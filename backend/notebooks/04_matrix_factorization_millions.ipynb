{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "j8oue55jrgo",
   "metadata": {},
   "source": [
    "# matrix factorization with millions of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eejxg7tpo8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 2 million ratings...\n",
      "loaded: 2,000,000 ratings\n",
      "users: 12,773\n",
      "movies: 36,603\n",
      "encoded - users: 12,773, movies: 36,603\n",
      "rating distribution:\n",
      "rating\n",
      "0.5     35379\n",
      "1.0     60992\n",
      "1.5     31635\n",
      "2.0    123674\n",
      "2.5    103883\n",
      "3.0    374510\n",
      "3.5    268803\n",
      "4.0    523517\n",
      "4.5    189480\n",
      "5.0    288127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load 2 million ratings\n",
    "print(\"loading 2 million ratings...\")\n",
    "ratings_df = pd.read_csv('../data/raw/ml-32m/ratings.csv', nrows=2000000)\n",
    "print(f\"loaded: {len(ratings_df):,} ratings\")\n",
    "print(f\"users: {ratings_df['userId'].nunique():,}\")\n",
    "print(f\"movies: {ratings_df['movieId'].nunique():,}\")\n",
    "\n",
    "# one number per user/movie\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings_df['user_idx'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "ratings_df['movie_idx'] = movie_encoder.fit_transform(ratings_df['movieId'])\n",
    "\n",
    "n_users = len(user_encoder.classes_)\n",
    "n_movies = len(movie_encoder.classes_)\n",
    "\n",
    "print(f\"encoded - users: {n_users:,}, movies: {n_movies:,}\")\n",
    "print(f\"rating distribution:\")\n",
    "print(ratings_df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "q9xumoqpci",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: 1,600,000 train, 400,000 test\n",
      "train tensors: torch.Size([1600000])\n",
      "test tensors: torch.Size([400000])\n",
      "rating range: 0.5 - 5.0\n",
      "average rating: 3.54\n"
     ]
    }
   ],
   "source": [
    "shuffled = ratings_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "split_idx = int(0.8 * len(shuffled))\n",
    "\n",
    "train_data = shuffled[:split_idx]\n",
    "test_data = shuffled[split_idx:]\n",
    "\n",
    "print(f\"split: {len(train_data):,} train, {len(test_data):,} test\")\n",
    "\n",
    "train_users = torch.tensor(train_data['user_idx'].values, dtype=torch.long)\n",
    "train_movies = torch.tensor(train_data['movie_idx'].values, dtype=torch.long)  \n",
    "train_ratings = torch.tensor(train_data['rating'].values, dtype=torch.float)\n",
    "\n",
    "test_users = torch.tensor(test_data['user_idx'].values, dtype=torch.long)\n",
    "test_movies = torch.tensor(test_data['movie_idx'].values, dtype=torch.long)\n",
    "test_ratings = torch.tensor(test_data['rating'].values, dtype=torch.float)\n",
    "\n",
    "print(f\"train tensors: {train_users.shape}\")\n",
    "print(f\"test tensors: {test_users.shape}\")\n",
    "print(f\"rating range: {train_ratings.min():.1f} - {train_ratings.max():.1f}\")\n",
    "print(f\"average rating: {train_ratings.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04d19bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_bias = train_ratings.mean()\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors=50, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.movie_factors = nn.Embedding(n_movies, n_factors)\n",
    "\n",
    "        # some users rate higher than others\n",
    "        self.user_biases = nn.Embedding(n_users, 1)\n",
    "\n",
    "        # some movies are rated higher than others\n",
    "        self.movie_biases = nn.Embedding(n_movies, 1)\n",
    "\n",
    "        # bias globally average rating\n",
    "        self.global_bias = nn.Parameter(torch.tensor(global_bias))\n",
    "\n",
    "        # less overfit\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.user_factors.weight.data.normal_(0, 0.1)\n",
    "        self.movie_factors.weight.data.normal_(0, 0.1)\n",
    "        self.user_biases.weight.data.normal_(0, 0.01)\n",
    "        self.movie_biases.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_vec = self.user_factors(user_ids)\n",
    "        movie_vec = self.movie_factors(movie_ids)\n",
    "        user_bias = self.user_biases(user_ids).squeeze()\n",
    "        movie_bias = self.movie_biases(movie_ids).squeeze()\n",
    "        \n",
    "        # apply dropout\n",
    "        user_vec = self.dropout(user_vec)\n",
    "        movie_vec = self.dropout(movie_vec)\n",
    "        \n",
    "        # using dot product + bias\n",
    "        dot_product = (user_vec * movie_vec).sum(dim=1)\n",
    "        raw_prediction = (\n",
    "            self.global_bias + \n",
    "            user_bias + \n",
    "            movie_bias + \n",
    "            dot_product\n",
    "        )\n",
    "        \n",
    "        return raw_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b195ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 12,773 users, 36,603 movies, embedding_dim=20\n",
      "total parameters: 1,036,897\n",
      "sample initial predictions: tensor([3.5087, 3.5877, 3.5377, 3.5739, 3.5612])\n",
      "sample actual ratings: tensor([4.0000, 4.0000, 3.0000, 4.0000, 3.5000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/x0hxzz0x5gd0mzmvr8l7q5_40000gp/T/ipykernel_23053/99656908.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.global_bias = nn.Parameter(torch.tensor(global_bias))\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_users=n_users, n_movies=n_movies, n_factors=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(f\"model: {n_users:,} users, {n_movies:,} movies, embedding_dim=20\")\n",
    "print(f\"total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# initial predictions\n",
    "with torch.no_grad():\n",
    "    initial_pred = model(train_users[:100], train_movies[:100])\n",
    "    print(f\"sample initial predictions: {initial_pred[:5]}\")\n",
    "    print(f\"sample actual ratings: {train_ratings[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "esvabbrv4hn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 1.1354\n",
      "epoch 0, loss: 1.1355\n",
      "epoch 20, loss: 0.7551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_movies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predictions, train_ratings)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/College/projects/monolith/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/College/projects/monolith/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m, in \u001b[0;36mMatrixFactorization.forward\u001b[0;34m(self, user_ids, movie_ids)\u001b[0m\n\u001b[1;32m     30\u001b[0m movie_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovie_biases(movie_ids)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# apply dropout\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m user_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m movie_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(movie_vec)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# using dot product + bias\u001b[39;00m\n",
      "File \u001b[0;32m~/College/projects/monolith/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/College/projects/monolith/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/College/projects/monolith/.venv/lib/python3.9/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/College/projects/monolith/.venv/lib/python3.9/site-packages/torch/nn/functional.py:1422\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1422\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"starting training...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    initial_pred = model(train_users, train_movies)\n",
    "    initial_loss = loss_fn(initial_pred, train_ratings)\n",
    "    print(f\"initial loss: {initial_loss:.4f}\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(train_users, train_movies)\n",
    "    loss = loss_fn(predictions, train_ratings)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"training done\")\n",
    "\n",
    "# evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred = model(train_users, train_movies)\n",
    "    test_pred = model(test_users, test_movies)\n",
    "    \n",
    "    train_loss = loss_fn(train_pred, train_ratings)\n",
    "    test_loss = loss_fn(test_pred, test_ratings)\n",
    "    test_rmse = torch.sqrt(test_loss)\n",
    "    \n",
    "    print(f\"\\nfinal results:\")\n",
    "    print(f\"train loss: {train_loss:.4f}\")\n",
    "    print(f\"test loss: {test_loss:.4f}\")\n",
    "    print(f\"test RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    print(f\"\\nsample predictions vs actual:\")\n",
    "    for i in range(10):\n",
    "        print(f\"predicted: {test_pred[i]:.2f}, actual: {test_ratings[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fi79iepq0y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 87,585 movies\n",
      "\n",
      "sample predictions with movie titles:\n",
      "user 4287: 'Mr. & Mrs. Smith (2005)'\n",
      "  predicted: 2.93, actual: 2.00\n",
      "\n",
      "user 2364: '20,000 Leagues Under the Sea (1954)'\n",
      "  predicted: 3.77, actual: 4.00\n",
      "\n",
      "user 2863: 'American Pie (1999)'\n",
      "  predicted: 3.73, actual: 4.00\n",
      "\n",
      "user 2679: 'Batman (1989)'\n",
      "  predicted: 3.70, actual: 3.00\n",
      "\n",
      "user 10584: 'Aladdin (1992)'\n",
      "  predicted: 3.31, actual: 3.50\n",
      "\n",
      "user 10639: 'Nymphomaniac: Volume II (2013)'\n",
      "  predicted: 5.20, actual: 5.00\n",
      "\n",
      "user 4167: 'Her (2013)'\n",
      "  predicted: 3.82, actual: 5.00\n",
      "\n",
      "user 10584: 'Twelve Monkeys (a.k.a. 12 Monkeys) (1995)'\n",
      "  predicted: 3.78, actual: 4.00\n",
      "\n",
      "user 6424: 'Scary Movie 4 (2006)'\n",
      "  predicted: 1.74, actual: 1.00\n",
      "\n",
      "user 8197: 'Kill Bill: Vol. 2 (2004)'\n",
      "  predicted: 3.42, actual: 3.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df = pd.read_csv('../data/raw/ml-32m/movies.csv')\n",
    "print(f\"loaded {len(movies_df):,} movies\")\n",
    "\n",
    "def get_movie_titles(movie_indices, movie_encoder, movies_df):\n",
    "    original_movie_ids = movie_encoder.inverse_transform(movie_indices.numpy())\n",
    "    titles = []\n",
    "    for movie_id in original_movie_ids:\n",
    "        movie_row = movies_df[movies_df['movieId'] == movie_id]\n",
    "        if len(movie_row) > 0:\n",
    "            titles.append(movie_row['title'].iloc[0])\n",
    "        else:\n",
    "            titles.append(\"unknown movie\")\n",
    "    return titles\n",
    "\n",
    "print(f\"\\nsample predictions with movie titles:\")\n",
    "sample_indices = range(10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_users_idx = test_users[sample_indices]\n",
    "    sample_movies_idx = test_movies[sample_indices]\n",
    "    sample_predictions = model(sample_users_idx, sample_movies_idx)\n",
    "    sample_actual = test_ratings[sample_indices]\n",
    "\n",
    "movie_titles = get_movie_titles(sample_movies_idx, movie_encoder, movies_df)\n",
    "original_user_ids = user_encoder.inverse_transform(sample_users_idx.numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"user {original_user_ids[i]}: '{movie_titles[i]}'\")\n",
    "    print(f\"  predicted: {sample_predictions[i]:.2f}, actual: {sample_actual[i]:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9dfc9",
   "metadata": {},
   "source": [
    "for myself:\n",
    "\n",
    "i think i've gotten to a pretty good RMSE with just collaborative filtering, but i think i should start to factor in content a bit. might add in genre data for each movie and account for users' genre preferences. same with movie year, but might break it down into decades rather than continuous years"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
